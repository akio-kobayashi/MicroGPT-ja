{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MicroGPT 入門ノート（日本語版）\n",
        "\n",
        "このノートブックは、わずか数百行で書かれた最小構成の GPT 実装を **初学者向け** に学ぶ教材です。\n",
        "学習・推論を実行しながら、損失や確率分布を可視化して動作を理解します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 謝辞\n",
        "\n",
        "この教材は Andrej Karpathy 氏の MicroGPT 実装をもとに、教育目的で日本語化・可視化を追加したものです。\n",
        "\n",
        "- Original: https://gist.github.com/karpathy/8627fe009c40f57531cb18360106ce95\n",
        "- Author: @karpathy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## この教材でできるようになること\n",
        "\n",
        "このノートを終えると、次を自分の言葉で説明できる状態を目標にします。\n",
        "\n",
        "1. 文字列がどのようにトークンIDへ変換されるか\n",
        "2. `Value` クラスで自動微分が成立する理由（局所勾配の連鎖）\n",
        "3. GPT の1ステップで「埋め込み→Attention→MLP→次トークン確率」がどう流れるか\n",
        "4. 損失が下がることと生成品質の関係\n",
        "\n",
        "学習の到達点を明確にするため、可視化セルを複数用意しています。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 前提知識（最低限）\n",
        "\n",
        "- Python の `for` 文とリスト内包表記\n",
        "- 中学レベルの指数・対数（`exp`, `log`）\n",
        "- 確率の基本（合計が1になる分布）\n",
        "\n",
        "数学が不安でも、セルを順に実行し「入力と出力」を確認すれば理解できるように構成しています。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 実行準備\n",
        "Colab ではそのまま動きます。必要なら以下を実行してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip -q install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "from typing import List\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 日本語サンプルデータ（self-contained）\n",
        "外部ファイルなしで学べるよう、簡単な日本語（ひらがな・カタカナ）サンプルを内蔵しています。\n",
        "\n",
        "必要に応じて `docs` を増やしてください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "docs = [\n",
        "    \"さくら\", \"すず\", \"はる\", \"ひなた\", \"みお\",\n",
        "    \"ゆい\", \"あかり\", \"りん\", \"あおい\", \"めい\",\n",
        "    \"たくみ\", \"そうた\", \"けん\", \"れん\", \"だいき\",\n",
        "    \"しょう\", \"こうた\", \"はると\", \"ゆう\", \"なお\",\n",
        "    \"サクラ\", \"ユイ\", \"アオイ\", \"レン\", \"ハルト\",\n",
        "    \"しおり\", \"かな\", \"まな\", \"りお\", \"えま\",\n",
        "]\n",
        "random.shuffle(docs)\n",
        "print(f\"文書数: {len(docs)}\")\n",
        "print(\"サンプル:\", docs[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. トークナイザ（文字単位）\n",
        "この最小実装では単語ではなく **1文字ずつ** をトークンとして扱います。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "uchars = sorted(set(''.join(docs)))\n",
        "BOS = len(uchars)\n",
        "vocab_size = len(uchars) + 1\n",
        "\n",
        "stoi = {ch: i for i, ch in enumerate(uchars)}\n",
        "itos = {i: ch for ch, i in stoi.items()}\n",
        "itos[BOS] = \"<BOS>\"\n",
        "\n",
        "print(f\"語彙数(vocab_size): {vocab_size}\")\n",
        "print(\"語彙:\", ''.join(uchars))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 文字列→ID 変換の具体例\n",
        "\n",
        "ここで実際に 1 語をトークン列にしてみます。  \n",
        "`<BOS>` は「文の開始/終了」を示す特別記号です。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "example = docs[0]\n",
        "encoded = [BOS] + [stoi[ch] for ch in example] + [BOS]\n",
        "decoded = ''.join(itos[i] for i in encoded if i != BOS)\n",
        "\n",
        "print('元の文字列:', example)\n",
        "print('ID列      :', encoded)\n",
        "print('復元文字列:', decoded)\n",
        "print('BOSのID   :', BOS)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 自動微分 Value クラス\n",
        "各値が『どの計算から生まれたか』を覚えておき、最後に `backward()` で勾配を逆伝播します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Value:\n",
        "    __slots__ = ('data', 'grad', '_children', '_local_grads')\n",
        "\n",
        "    def __init__(self, data, children=(), local_grads=()):\n",
        "        # 順伝播で得られる実数値\n",
        "        self.data = data\n",
        "        # 逆伝播で蓄積される勾配\n",
        "        self.grad = 0\n",
        "        # 計算グラフ上の親ノード\n",
        "        self._children = children\n",
        "        # 親に対する局所勾配\n",
        "        self._local_grads = local_grads\n",
        "\n",
        "    def __add__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        return Value(self.data + other.data, (self, other), (1, 1))\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        return Value(self.data * other.data, (self, other), (other.data, self.data))\n",
        "\n",
        "    def __pow__(self, other):\n",
        "        return Value(self.data ** other, (self,), (other * self.data ** (other - 1),))\n",
        "\n",
        "    def log(self):\n",
        "        return Value(math.log(self.data), (self,), (1 / self.data,))\n",
        "\n",
        "    def exp(self):\n",
        "        return Value(math.exp(self.data), (self,), (math.exp(self.data),))\n",
        "\n",
        "    def relu(self):\n",
        "        return Value(max(0, self.data), (self,), (float(self.data > 0),))\n",
        "\n",
        "    def __neg__(self): return self * -1\n",
        "    def __radd__(self, other): return self + other\n",
        "    def __sub__(self, other): return self + (-other)\n",
        "    def __rsub__(self, other): return other + (-self)\n",
        "    def __rmul__(self, other): return self * other\n",
        "    def __truediv__(self, other): return self * other ** -1\n",
        "    def __rtruediv__(self, other): return other * self ** -1\n",
        "\n",
        "    def backward(self):\n",
        "        # 計算グラフをトポロジカル順序に並べる\n",
        "        topo = []\n",
        "        visited = set()\n",
        "\n",
        "        def build_topo(v):\n",
        "            if v not in visited:\n",
        "                visited.add(v)\n",
        "                for child in v._children:\n",
        "                    build_topo(child)\n",
        "                topo.append(v)\n",
        "\n",
        "        build_topo(self)\n",
        "        self.grad = 1\n",
        "\n",
        "        # 逆順にたどって勾配を伝播\n",
        "        for v in reversed(topo):\n",
        "            for child, local_grad in zip(v._children, v._local_grads):\n",
        "                child.grad += local_grad * v.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 逆伝播の最小例（手計算対応）\n",
        "\n",
        "次の式の勾配を確認します。  \n",
        "`y = (a * b + c)^2` のとき、`dy/da`, `dy/db`, `dy/dc` がどうなるかを比較します。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "a = Value(2.0)\n",
        "b = Value(3.0)\n",
        "c = Value(1.0)\n",
        "y = (a * b + c) ** 2\n",
        "y.backward()\n",
        "\n",
        "print('y =', y.data)\n",
        "print('dy/da =', a.grad)\n",
        "print('dy/db =', b.grad)\n",
        "print('dy/dc =', c.grad)\n",
        "\n",
        "# 手計算: (2*3+1)^2 = 49, dy/dx=2x=14, x=ab+c\n",
        "# dy/da = 14*b = 42, dy/db = 14*a = 28, dy/dc = 14\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. モデル定義（ミニGPT）\n",
        "計算量を抑えるため、教育向けの小さな設定を使います。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===== ハイパーパラメータ =====\n",
        "n_embd = 16\n",
        "n_head = 4\n",
        "n_layer = 1\n",
        "block_size = 16\n",
        "head_dim = n_embd // n_head\n",
        "\n",
        "def matrix(nout, nin, std=0.08):\n",
        "    return [[Value(random.gauss(0, std)) for _ in range(nin)] for _ in range(nout)]\n",
        "\n",
        "state_dict = {\n",
        "    'wte': matrix(vocab_size, n_embd),\n",
        "    'wpe': matrix(block_size, n_embd),\n",
        "    'lm_head': matrix(vocab_size, n_embd),\n",
        "}\n",
        "\n",
        "for i in range(n_layer):\n",
        "    state_dict[f'layer{i}.attn_wq'] = matrix(n_embd, n_embd)\n",
        "    state_dict[f'layer{i}.attn_wk'] = matrix(n_embd, n_embd)\n",
        "    state_dict[f'layer{i}.attn_wv'] = matrix(n_embd, n_embd)\n",
        "    state_dict[f'layer{i}.attn_wo'] = matrix(n_embd, n_embd)\n",
        "    state_dict[f'layer{i}.mlp_fc1'] = matrix(4 * n_embd, n_embd)\n",
        "    state_dict[f'layer{i}.mlp_fc2'] = matrix(n_embd, 4 * n_embd)\n",
        "\n",
        "params = [p for mat in state_dict.values() for row in mat for p in row]\n",
        "print(f\"パラメータ数: {len(params)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def linear(x, w):\n",
        "    return [sum(wi * xi for wi, xi in zip(wo, x)) for wo in w]\n",
        "\n",
        "def softmax(logits):\n",
        "    max_val = max(val.data for val in logits)\n",
        "    exps = [(val - max_val).exp() for val in logits]\n",
        "    total = sum(exps)\n",
        "    return [e / total for e in exps]\n",
        "\n",
        "def rmsnorm(x):\n",
        "    ms = sum(xi * xi for xi in x) / len(x)\n",
        "    scale = (ms + 1e-5) ** -0.5\n",
        "    return [xi * scale for xi in x]\n",
        "\n",
        "def gpt(token_id, pos_id, keys, values):\n",
        "    tok_emb = state_dict['wte'][token_id]\n",
        "    pos_emb = state_dict['wpe'][pos_id]\n",
        "    x = [t + p for t, p in zip(tok_emb, pos_emb)]\n",
        "    x = rmsnorm(x)\n",
        "\n",
        "    for li in range(n_layer):\n",
        "        x_residual = x\n",
        "        x = rmsnorm(x)\n",
        "        q = linear(x, state_dict[f'layer{li}.attn_wq'])\n",
        "        k = linear(x, state_dict[f'layer{li}.attn_wk'])\n",
        "        v = linear(x, state_dict[f'layer{li}.attn_wv'])\n",
        "        keys[li].append(k)\n",
        "        values[li].append(v)\n",
        "\n",
        "        x_attn = []\n",
        "        for h in range(n_head):\n",
        "            hs = h * head_dim\n",
        "            q_h = q[hs:hs + head_dim]\n",
        "            k_h = [ki[hs:hs + head_dim] for ki in keys[li]]\n",
        "            v_h = [vi[hs:hs + head_dim] for vi in values[li]]\n",
        "\n",
        "            attn_logits = [\n",
        "                sum(q_h[j] * k_h[t][j] for j in range(head_dim)) / head_dim ** 0.5\n",
        "                for t in range(len(k_h))\n",
        "            ]\n",
        "            attn_weights = softmax(attn_logits)\n",
        "            head_out = [\n",
        "                sum(attn_weights[t] * v_h[t][j] for t in range(len(v_h)))\n",
        "                for j in range(head_dim)\n",
        "            ]\n",
        "            x_attn.extend(head_out)\n",
        "\n",
        "        x = linear(x_attn, state_dict[f'layer{li}.attn_wo'])\n",
        "        x = [a + b for a, b in zip(x, x_residual)]\n",
        "\n",
        "        x_residual = x\n",
        "        x = rmsnorm(x)\n",
        "        x = linear(x, state_dict[f'layer{li}.mlp_fc1'])\n",
        "        x = [xi.relu() for xi in x]\n",
        "        x = linear(x, state_dict[f'layer{li}.mlp_fc2'])\n",
        "        x = [a + b for a, b in zip(x, x_residual)]\n",
        "\n",
        "    logits = linear(x, state_dict['lm_head'])\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.5 データがモデル内をどう流れるか（1時刻分）\n",
        "\n",
        "1時刻 `t` では、次の順で計算されます。\n",
        "\n",
        "1. 入力ID `token_id` と位置 `pos_id` から埋め込みを作る\n",
        "2. 過去までの `key/value` と現在の `query` で注意重みを計算\n",
        "3. 注意出力を MLP に通して表現を更新\n",
        "4. 線形層 `lm_head` で語彙サイズ分の `logits` を得る\n",
        "5. `softmax` で確率に変換し、正解IDの負の対数を損失にする\n",
        "\n",
        "この「次文字予測」を全時刻で平均したものが最終損失です。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 学習\n",
        "学習ログを保存し、あとで損失曲線を描きます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_rate, beta1, beta2, eps_adam = 0.01, 0.85, 0.99, 1e-8\n",
        "m = [0.0] * len(params)\n",
        "v = [0.0] * len(params)\n",
        "\n",
        "num_steps = 600\n",
        "loss_history: List[float] = []\n",
        "\n",
        "for step in range(num_steps):\n",
        "    doc = docs[step % len(docs)]\n",
        "    tokens = [BOS] + [stoi[ch] for ch in doc] + [BOS]\n",
        "    n = min(block_size, len(tokens) - 1)\n",
        "\n",
        "    keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\n",
        "    losses = []\n",
        "    for pos_id in range(n):\n",
        "        token_id, target_id = tokens[pos_id], tokens[pos_id + 1]\n",
        "        logits = gpt(token_id, pos_id, keys, values)\n",
        "        probs = softmax(logits)\n",
        "        loss_t = -probs[target_id].log()\n",
        "        losses.append(loss_t)\n",
        "\n",
        "    loss = (1 / n) * sum(losses)\n",
        "    loss.backward()\n",
        "\n",
        "    lr_t = learning_rate * (1 - step / num_steps)\n",
        "    for i, p in enumerate(params):\n",
        "        m[i] = beta1 * m[i] + (1 - beta1) * p.grad\n",
        "        v[i] = beta2 * v[i] + (1 - beta2) * p.grad ** 2\n",
        "        m_hat = m[i] / (1 - beta1 ** (step + 1))\n",
        "        v_hat = v[i] / (1 - beta2 ** (step + 1))\n",
        "        p.data -= lr_t * m_hat / (v_hat ** 0.5 + eps_adam)\n",
        "        p.grad = 0\n",
        "\n",
        "    loss_history.append(loss.data)\n",
        "\n",
        "    if (step + 1) % 50 == 0:\n",
        "        print(f\"step {step+1:4d}/{num_steps} | loss={loss.data:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(loss_history, label='train loss')\n",
        "plt.title('学習損失の推移')\n",
        "plt.xlabel('step')\n",
        "plt.ylabel('loss')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. 推論（新しい名前を生成）\n",
        "温度 `temperature` を上げると多様性が増え、下げると安定します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sample_one(temperature=0.8, max_len=block_size):\n",
        "    keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\n",
        "    token_id = BOS\n",
        "    out = []\n",
        "\n",
        "    for pos_id in range(max_len):\n",
        "        logits = gpt(token_id, pos_id, keys, values)\n",
        "        probs = softmax([l / temperature for l in logits])\n",
        "        token_id = random.choices(range(vocab_size), weights=[p.data for p in probs])[0]\n",
        "        if token_id == BOS:\n",
        "            break\n",
        "        out.append(itos[token_id])\n",
        "\n",
        "    return ''.join(out)\n",
        "\n",
        "for i in range(12):\n",
        "    print(f\"sample {i+1:2d}: {sample_one(temperature=0.7)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. 可視化: Attention 重み（1ヘッド）\n",
        "\n",
        "どの位置をどれだけ参照したかを見ます。  \n",
        "簡易版として、先頭ヘッドの重みをヒートマップ表示します。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def attention_weights_for_text(text, head=0):\n",
        "    tokens = [BOS] + [stoi[ch] for ch in text]\n",
        "    keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\n",
        "    ws = []\n",
        "\n",
        "    for pos_id, token_id in enumerate(tokens[:block_size]):\n",
        "        tok_emb = state_dict['wte'][token_id]\n",
        "        pos_emb = state_dict['wpe'][pos_id]\n",
        "        x = [t + p for t, p in zip(tok_emb, pos_emb)]\n",
        "        x = rmsnorm(x)\n",
        "\n",
        "        li = 0\n",
        "        x_norm = rmsnorm(x)\n",
        "        q = linear(x_norm, state_dict[f'layer{li}.attn_wq'])\n",
        "        k = linear(x_norm, state_dict[f'layer{li}.attn_wk'])\n",
        "        v = linear(x_norm, state_dict[f'layer{li}.attn_wv'])\n",
        "        keys[li].append(k)\n",
        "        values[li].append(v)\n",
        "\n",
        "        hs = head * head_dim\n",
        "        q_h = q[hs:hs + head_dim]\n",
        "        k_h = [ki[hs:hs + head_dim] for ki in keys[li]]\n",
        "        logits = [sum(q_h[j] * k_h[t][j] for j in range(head_dim)) / (head_dim ** 0.5) for t in range(len(k_h))]\n",
        "        aw = softmax(logits)\n",
        "        ws.append([a.data for a in aw])\n",
        "\n",
        "    return ws, tokens\n",
        "\n",
        "probe = docs[0]\n",
        "weights, tokens = attention_weights_for_text(probe, head=0)\n",
        "\n",
        "# 可視化しやすいように正方行列へ（未来側は0埋め）\n",
        "L = len(weights)\n",
        "mat = [[0.0 for _ in range(L)] for _ in range(L)]\n",
        "for i in range(L):\n",
        "    for j in range(i + 1):\n",
        "        mat[i][j] = weights[i][j]\n",
        "\n",
        "labels = ['<BOS>'] + list(probe[:L-1])\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.imshow(mat, cmap='Blues')\n",
        "plt.colorbar()\n",
        "plt.xticks(range(L), labels, rotation=45)\n",
        "plt.yticks(range(L), labels)\n",
        "plt.title(f'Attention重み (head=0) / text=\"{probe}\"')\n",
        "plt.xlabel('参照位置')\n",
        "plt.ylabel('現在位置')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. 可視化: 次トークン確率 Top-10\n",
        "先頭トークン（`<BOS>`）から1文字目として何が出やすいかを棒グラフで確認します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\n",
        "logits = gpt(BOS, 0, keys, values)\n",
        "probs = softmax(logits)\n",
        "prob_values = [p.data for p in probs]\n",
        "\n",
        "topk = 10\n",
        "top_ids = sorted(range(vocab_size), key=lambda i: prob_values[i], reverse=True)[:topk]\n",
        "top_labels = [itos[i] for i in top_ids]\n",
        "top_probs = [prob_values[i] for i in top_ids]\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.bar(top_labels, top_probs)\n",
        "plt.title('次トークン確率 Top-10 (開始トークンから)')\n",
        "plt.xlabel('トークン')\n",
        "plt.ylabel('確率')\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "for t, p in zip(top_labels, top_probs):\n",
        "    print(f\"{t:>5}: {p:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "MicroGPT_ja_colab.ipynb"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}